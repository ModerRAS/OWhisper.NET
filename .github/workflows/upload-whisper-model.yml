name: ä¸‹è½½å¹¶ä¸Šä¼  Whisper Large V3 Turbo æ¨¡å‹

on:
  workflow_dispatch:
    inputs:
      model_version:
        description: 'æ¨¡å‹ç‰ˆæœ¬ (é»˜è®¤: large-v3-turbo)'
        required: false
        default: 'large-v3-turbo'
        type: string
      force_download:
        description: 'å¼ºåˆ¶é‡æ–°ä¸‹è½½æ¨¡å‹'
        required: false
        default: false
        type: boolean

jobs:
  download-and-upload-model:
    permissions: write-all
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      
    - name: è®¾ç½® Python ç¯å¢ƒ
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: å®‰è£…ä¾èµ–åŒ…
      run: |
        pip install huggingface_hub torch transformers boto3
        
    - name: æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨å¹¶ä¸‹è½½ä¸Šä¼ 
      env:
        R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
        R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        S3_ENDPOINT: ${{ secrets.S3_ENDPOINT }}
        R2_BUCKET: ${{ secrets.R2_BUCKET }}
        MODEL_VERSION: ${{ github.event.inputs.model_version }}
        FORCE_DOWNLOAD: ${{ github.event.inputs.force_download }}
        WORKFLOW_NAME: ${{ github.workflow }}
        RUN_ID: ${{ github.run_id }}
        RUN_NUMBER: ${{ github.run_number }}
      run: |
        python << 'EOF'
        import os
        import json
        import hashlib
        import urllib.request
        import urllib.error
        from datetime import datetime, timezone
        from pathlib import Path
        import boto3
        from botocore.exceptions import ClientError, NoCredentialsError
        from huggingface_hub import hf_hub_download
        from transformers import WhisperProcessor, WhisperForConditionalGeneration
        import shutil

        # ç¯å¢ƒå˜é‡
        model_version = os.environ['MODEL_VERSION']
        force_download = os.environ['FORCE_DOWNLOAD'].lower() == 'true'
        
        # R2 é…ç½®
        r2_access_key = os.environ['R2_ACCESS_KEY_ID']
        r2_secret_key = os.environ['R2_SECRET_ACCESS_KEY']
        s3_endpoint = os.environ['S3_ENDPOINT']
        bucket_name = os.environ['R2_BUCKET']
        
        # å·¥ä½œæµä¿¡æ¯
        workflow_name = os.environ['WORKFLOW_NAME']
        run_id = os.environ['RUN_ID']
        run_number = os.environ['RUN_NUMBER']

        print(f"å¼€å§‹å¤„ç†æ¨¡å‹: {model_version}")
        print(f"å¼ºåˆ¶é‡æ–°ä¸‹è½½: {force_download}")

        # åˆ›å»º S3 å®¢æˆ·ç«¯
        try:
            s3_client = boto3.client(
                's3',
                endpoint_url=s3_endpoint,
                aws_access_key_id=r2_access_key,
                aws_secret_access_key=r2_secret_key,
                region_name='auto'
            )
            print("âœ… S3 å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ åˆ›å»º S3 å®¢æˆ·ç«¯å¤±è´¥: {e}")
            exit(1)

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
        model_filename = f"ggml-{model_version}.bin"
        model_key = f"models/{model_filename}"
        
        model_exists = False
        try:
            s3_client.head_object(Bucket=bucket_name, Key=model_key)
            model_exists = True
            print(f"âœ… æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨äº R2: {model_key}")
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                print(f"ğŸ“¥ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸‹è½½: {model_key}")
            else:
                print(f"âš ï¸  æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        # å†³å®šæ˜¯å¦éœ€è¦ä¸‹è½½
        should_download = not model_exists or force_download
        
        if not should_download:
            print("â­ï¸  è·³è¿‡ä¸‹è½½ï¼Œä½¿ç”¨ç°æœ‰æ–‡ä»¶")
            exit(0)
        
        print("ğŸš€ å¼€å§‹ä¸‹è½½æ¨¡å‹...")

        # åˆ›å»ºæ¨¡å‹ä¸‹è½½ç›®å½•
        models_dir = Path("models")
        models_dir.mkdir(exist_ok=True)
        
        # æ ¹æ®æ¨¡å‹ç‰ˆæœ¬è®¾ç½®ä»“åº“å’Œæ–‡ä»¶å
        model_repo_map = {
            "large-v3-turbo": "openai/whisper-large-v3-turbo",
            "large-v3": "openai/whisper-large-v3",
            "large-v2": "openai/whisper-large-v2",
            "large": "openai/whisper-large",
            "medium": "openai/whisper-medium",
            "small": "openai/whisper-small",
            "base": "openai/whisper-base",
            "tiny": "openai/whisper-tiny"
        }
        
        # GGUF æ ¼å¼æ¨¡å‹çš„ç›´æ¥ä¸‹è½½é“¾æ¥
        gguf_download_urls = {
            "large-v3-turbo": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3-turbo.bin",
                "https://huggingface.co/Demonthos/candle-quantized-whisper-large-v3-turbo/resolve/main/model.gguf"
            ],
            "large-v3": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3.bin"
            ],
            "large-v2": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v2.bin"
            ],
            "large": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large.bin"
            ],
            "medium": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-medium.bin"
            ],
            "small": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-small.bin"
            ],
            "base": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.bin"
            ],
            "tiny": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.bin"
            ]
        }
        
        output_path = models_dir / model_filename
        download_success = False
        
        # å°è¯•ç›´æ¥ä¸‹è½½ GGUF æ ¼å¼çš„æ¨¡å‹
        if model_version in gguf_download_urls:
            for url in gguf_download_urls[model_version]:
                try:
                    print(f"ğŸ“¥ æ­£åœ¨ä» {url} ä¸‹è½½æ¨¡å‹...")
                    urllib.request.urlretrieve(url, output_path)
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸‹è½½æˆåŠŸ
                    if output_path.exists() and output_path.stat().st_size > 1024*1024:  # è‡³å°‘1MB
                        print(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {output_path}")
                        file_size = output_path.stat().st_size
                        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size / (1024*1024*1024):.2f} GB")
                        download_success = True
                        break
                    else:
                        print(f"âŒ ä» {url} ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªURL...")
                        if output_path.exists():
                            output_path.unlink()
                        
                except Exception as e:
                    print(f"âŒ ä» {url} ä¸‹è½½å¤±è´¥: {str(e)}")
                    if output_path.exists():
                        output_path.unlink()
                    continue
        
        # å¦‚æœç›´æ¥ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä» Hugging Face Hub ä¸‹è½½å¹¶è½¬æ¢
        if not download_success and model_version in model_repo_map:
            try:
                print("ğŸ“¦ å°è¯•ä» Hugging Face Hub ä¸‹è½½åŸå§‹æ¨¡å‹...")
                repo_id = model_repo_map[model_version]
                
                print(f"ğŸ”„ æ­£åœ¨ä¸‹è½½ {repo_id}...")
                processor = WhisperProcessor.from_pretrained(repo_id)
                model = WhisperForConditionalGeneration.from_pretrained(repo_id)
                
                # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜æ¨¡å‹
                temp_dir = Path(f"temp_model_{model_version}")
                temp_dir.mkdir(exist_ok=True)
                
                # ä¿å­˜æ¨¡å‹
                model.save_pretrained(temp_dir)
                processor.save_pretrained(temp_dir)
                
                print("âš ï¸  æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œä½†éœ€è¦æ‰‹åŠ¨è½¬æ¢ä¸º GGUF æ ¼å¼")
                print("â„¹ï¸  æ³¨æ„ï¼šè¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch æ ¼å¼ï¼Œéœ€è¦ä½¿ç”¨ whisper.cpp å·¥å…·è½¬æ¢ä¸º GGUF æ ¼å¼")
                
                # æ¸…ç†ä¸´æ—¶ç›®å½•
                shutil.rmtree(temp_dir, ignore_errors=True)
                
                # åˆ›å»ºä¸€ä¸ªå ä½ç¬¦æ–‡ä»¶è¯´æ˜æƒ…å†µ
                with open(output_path, 'w') as f:
                    f.write(f"æ¨¡å‹ {model_version} å·²ä» {repo_id} ä¸‹è½½ï¼Œä½†éœ€è¦è½¬æ¢ä¸º GGUF æ ¼å¼ã€‚\n")
                    f.write("è¯·ä½¿ç”¨ whisper.cpp å·¥å…·è¿›è¡Œè½¬æ¢ã€‚\n")
                
                download_success = True
                
            except Exception as e:
                print(f"âŒ ä» Hugging Face Hub ä¸‹è½½å¤±è´¥: {str(e)}")
        
        if not download_success:
            print(f"âŒ æ— æ³•ä¸‹è½½æ¨¡å‹ {model_version}ï¼Œæ‰€æœ‰ä¸‹è½½æ–¹å¼éƒ½å¤±è´¥äº†")
            exit(1)

        # éªŒè¯ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶
        print("ğŸ” éªŒè¯ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶...")
        
        if not output_path.exists():
            print("âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸‹è½½å¤±è´¥")
            exit(1)
            
        file_size = output_path.stat().st_size
        print(f"âœ… æ¨¡å‹æ–‡ä»¶éªŒè¯æˆåŠŸ:")
        print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {output_path}")
        print(f"   ğŸ“ æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.1f} MB")
        
        # è®¡ç®— SHA256 æ ¡éªŒå’Œ
        sha256_hash = ""
        if file_size > 1048576:  # å¤§äº1MB
            print("ğŸ” è®¡ç®— SHA256 æ ¡éªŒå’Œ...")
            hash_sha256 = hashlib.sha256()
            with open(output_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            sha256_hash = hash_sha256.hexdigest()
            print(f"   ğŸ”‘ SHA256: {sha256_hash}")
            
            # ä¿å­˜æ ¡éªŒå’Œæ–‡ä»¶
            sha256_file = output_path.with_suffix('.bin.sha256')
            with open(sha256_file, 'w') as f:
                f.write(f"{sha256_hash}  {model_filename}\n")
        else:
            print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶å¤§å°å¯èƒ½ä¸æ­£ç¡® ({file_size} bytes)")

        # ä¸Šä¼ æ¨¡å‹åˆ° Cloudflare R2
        print("â˜ï¸  å¼€å§‹ä¸Šä¼ æ¨¡å‹æ–‡ä»¶åˆ° Cloudflare R2...")
        
        try:
            # ä¸Šä¼ æ¨¡å‹æ–‡ä»¶
            print(f"ğŸ“¤ ä¸Šä¼ æ¨¡å‹æ–‡ä»¶: {model_filename}")
            with open(output_path, 'rb') as f:
                s3_client.upload_fileobj(f, bucket_name, model_key)
            print("âœ… æ¨¡å‹æ–‡ä»¶ä¸Šä¼ å®Œæˆ")
            
            # ä¸Šä¼ æ ¡éªŒå’Œæ–‡ä»¶
            sha256_file = output_path.with_suffix('.bin.sha256')
            if sha256_file.exists():
                sha256_key = f"models/{model_filename}.sha256"
                print(f"ğŸ“¤ ä¸Šä¼ æ ¡éªŒå’Œæ–‡ä»¶: {model_filename}.sha256")
                with open(sha256_file, 'rb') as f:
                    s3_client.upload_fileobj(f, bucket_name, sha256_key)
                print("âœ… æ ¡éªŒå’Œæ–‡ä»¶ä¸Šä¼ å®Œæˆ")
                
        except Exception as e:
            print(f"âŒ ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {e}")
            exit(1)

        # åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶
        print("ğŸ“‹ åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶...")
        
        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        def format_file_size(size_bytes):
            if size_bytes >= 1024**3:
                return f"{size_bytes / (1024**3):.1f}G"
            elif size_bytes >= 1024**2:
                return f"{size_bytes / (1024**2):.1f}M"
            elif size_bytes >= 1024:
                return f"{size_bytes / 1024:.1f}K"
            else:
                return f"{size_bytes}B"
        
        model_info = {
            "model_name": f"whisper-{model_version}",
            "version": model_version,
            "format": "gguf",
            "file_name": model_filename,
            "uploaded_at": datetime.now(timezone.utc).isoformat(),
            "upload_workflow": workflow_name,
            "run_id": run_id,
            "run_number": run_number,
            "file_size_bytes": file_size,
            "file_size_human": format_file_size(file_size),
            "sha256": sha256_hash if sha256_hash else "unknown",
            "download_url": f"https://velopack.miaostay.com/models/{model_filename}",
            "description": f"Whisper {model_version} GGUF format model for speech recognition",
            "compatible_with": ["whisper.cpp", "llama.cpp", "candle-whisper"],
            "license": "MIT",
            "source": "OpenAI Whisper"
        }
        
        print("ğŸ“„ æ¨¡å‹ä¿¡æ¯æ–‡ä»¶å†…å®¹:")
        print(json.dumps(model_info, indent=2, ensure_ascii=False))
        
        # ä¸Šä¼ æ¨¡å‹ä¿¡æ¯æ–‡ä»¶
        try:
            info_key = f"models/whisper-{model_version}-info.json"
            info_content = json.dumps(model_info, indent=2, ensure_ascii=False)
            s3_client.put_object(
                Bucket=bucket_name,
                Key=info_key,
                Body=info_content.encode('utf-8'),
                ContentType='application/json'
            )
            print("âœ… æ¨¡å‹ä¿¡æ¯æ–‡ä»¶ä¸Šä¼ å®Œæˆ")
        except Exception as e:
            print(f"âŒ ä¸Šä¼ æ¨¡å‹ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {e}")

        # åˆ—å‡º R2 ä¸­çš„æ¨¡å‹æ–‡ä»¶
        print("ğŸ“‚ å½“å‰ R2 å­˜å‚¨æ¡¶ä¸­çš„æ¨¡å‹æ–‡ä»¶:")
        try:
            response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix="models/")
            if 'Contents' in response:
                for obj in response['Contents']:
                    size_human = format_file_size(obj['Size'])
                    print(f"   ğŸ“„ {obj['Key']} ({size_human})")
            else:
                print("   ğŸ“­ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        except Exception as e:
            print(f"âš ï¸  åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")

        # ç”Ÿæˆ GitHub Actions æ‘˜è¦
        summary_lines = [
            "## ğŸ‰ Whisper æ¨¡å‹ä¸Šä¼ å®Œæˆ",
            "",
            "### ğŸ“‹ ä¸Šä¼ è¯¦æƒ…",
            f"- **æ¨¡å‹ç‰ˆæœ¬**: {model_version}",
            f"- **ä¸Šä¼ æ—¶é—´**: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}",
            f"- **å·¥ä½œæµ**: {workflow_name}",
            f"- **è¿è¡Œ ID**: {run_id}",
            f"- **è¿è¡Œç¼–å·**: {run_number}",
            f"- **æ“ä½œ**: {'å¼ºåˆ¶é‡æ–°ä¸‹è½½å¹¶ä¸Šä¼ ' if force_download else 'ä¸‹è½½å¹¶ä¸Šä¼ æ–°æ¨¡å‹'}",
            "",
            "### ğŸ“ æ–‡ä»¶ä½ç½®",
            f"- **æ¨¡å‹æ–‡ä»¶**: `models/{model_filename}`",
            f"- **æ ¡éªŒå’Œ**: `models/{model_filename}.sha256`",
            f"- **ä¿¡æ¯æ–‡ä»¶**: `models/whisper-{model_version}-info.json`",
            "",
            "### ğŸ“Š æ–‡ä»¶ä¿¡æ¯",
            f"- **æ–‡ä»¶å¤§å°**: {format_file_size(file_size)}",
            f"- **SHA256**: `{sha256_hash if sha256_hash else 'unknown'}`",
            "",
            "### ğŸ”— ä½¿ç”¨è¯´æ˜",
            "æ¨¡å‹æ–‡ä»¶å·²ä¸Šä¼ åˆ° Cloudflare R2ï¼Œå¯ä»¥åœ¨æ‚¨çš„åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ä»¥ä¸‹ URL è®¿é—®:",
            "```",
            f"https://velopack.miaostay.com/models/{model_filename}",
            "```",
            "",
            "### ğŸ› ï¸ å…¼å®¹æ€§",
            "è¿™ä¸ªæ¨¡å‹ä¸ä»¥ä¸‹å·¥å…·å…¼å®¹:",
            "- whisper.cpp",
            "- llama.cpp", 
            "- candle-whisper",
            "- å…¶ä»–æ”¯æŒ GGUF æ ¼å¼çš„æ¨ç†å¼•æ“"
        ]
        
        # å†™å…¥ GitHub Actions æ‘˜è¦
        summary_file = os.environ.get('GITHUB_STEP_SUMMARY')
        if summary_file:
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(summary_lines))
            print("ğŸ“ GitHub Actions æ‘˜è¦å·²ç”Ÿæˆ")
        
        print("ğŸŠ æ¨¡å‹å¤„ç†å®Œæˆï¼")
        
        EOF 