name: 下载并上传 Whisper Large V3 Turbo 模型

on:
  workflow_dispatch:
    inputs:
      model_version:
        description: '模型版本 (默认: large-v3-turbo)'
        required: false
        default: 'large-v3-turbo'
        type: string
      force_download:
        description: '强制重新下载模型'
        required: false
        default: false
        type: boolean

jobs:
  download-and-upload-model:
    permissions: write-all
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      
    - name: 设置 Python 环境
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: 安装依赖包
      run: |
        pip install huggingface_hub torch transformers boto3
        
    - name: 检查模型是否已存在并下载上传
      env:
        R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
        R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        S3_ENDPOINT: ${{ secrets.S3_ENDPOINT }}
        R2_BUCKET: ${{ secrets.R2_BUCKET }}
        MODEL_VERSION: ${{ github.event.inputs.model_version }}
        FORCE_DOWNLOAD: ${{ github.event.inputs.force_download }}
        WORKFLOW_NAME: ${{ github.workflow }}
        RUN_ID: ${{ github.run_id }}
        RUN_NUMBER: ${{ github.run_number }}
      run: |
        python << 'EOF'
        import os
        import json
        import hashlib
        import urllib.request
        import urllib.error
        from datetime import datetime, timezone
        from pathlib import Path
        import boto3
        from botocore.exceptions import ClientError, NoCredentialsError
        from huggingface_hub import hf_hub_download
        from transformers import WhisperProcessor, WhisperForConditionalGeneration
        import shutil

        # 环境变量
        model_version = os.environ['MODEL_VERSION']
        force_download = os.environ['FORCE_DOWNLOAD'].lower() == 'true'
        
        # R2 配置
        r2_access_key = os.environ['R2_ACCESS_KEY_ID']
        r2_secret_key = os.environ['R2_SECRET_ACCESS_KEY']
        s3_endpoint = os.environ['S3_ENDPOINT']
        bucket_name = os.environ['R2_BUCKET']
        
        # 工作流信息
        workflow_name = os.environ['WORKFLOW_NAME']
        run_id = os.environ['RUN_ID']
        run_number = os.environ['RUN_NUMBER']

        print(f"开始处理模型: {model_version}")
        print(f"强制重新下载: {force_download}")

        # 创建 S3 客户端
        try:
            s3_client = boto3.client(
                's3',
                endpoint_url=s3_endpoint,
                aws_access_key_id=r2_access_key,
                aws_secret_access_key=r2_secret_key,
                region_name='auto'
            )
            print("✅ S3 客户端创建成功")
        except Exception as e:
            print(f"❌ 创建 S3 客户端失败: {e}")
            exit(1)

        # 检查模型是否已存在
        model_filename = f"ggml-{model_version}.bin"
        model_key = f"models/{model_filename}"
        
        model_exists = False
        try:
            s3_client.head_object(Bucket=bucket_name, Key=model_key)
            model_exists = True
            print(f"✅ 模型文件已存在于 R2: {model_key}")
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                print(f"📥 模型文件不存在，需要下载: {model_key}")
            else:
                print(f"⚠️  检查模型文件时出错: {e}")

        # 决定是否需要下载
        should_download = not model_exists or force_download
        
        if not should_download:
            print("⏭️  跳过下载，使用现有文件")
            exit(0)
        
        print("🚀 开始下载模型...")

        # 创建模型下载目录
        models_dir = Path("models")
        models_dir.mkdir(exist_ok=True)
        
        # 根据模型版本设置仓库和文件名
        model_repo_map = {
            "large-v3-turbo": "openai/whisper-large-v3-turbo",
            "large-v3": "openai/whisper-large-v3",
            "large-v2": "openai/whisper-large-v2",
            "large": "openai/whisper-large",
            "medium": "openai/whisper-medium",
            "small": "openai/whisper-small",
            "base": "openai/whisper-base",
            "tiny": "openai/whisper-tiny"
        }
        
        # GGUF 格式模型的直接下载链接
        gguf_download_urls = {
            "large-v3-turbo": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3-turbo.bin",
                "https://huggingface.co/Demonthos/candle-quantized-whisper-large-v3-turbo/resolve/main/model.gguf"
            ],
            "large-v3": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3.bin"
            ],
            "large-v2": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v2.bin"
            ],
            "large": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large.bin"
            ],
            "medium": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-medium.bin"
            ],
            "small": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-small.bin"
            ],
            "base": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.bin"
            ],
            "tiny": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.bin"
            ]
        }
        
        output_path = models_dir / model_filename
        download_success = False
        
        # 尝试直接下载 GGUF 格式的模型
        if model_version in gguf_download_urls:
            for url in gguf_download_urls[model_version]:
                try:
                    print(f"📥 正在从 {url} 下载模型...")
                    urllib.request.urlretrieve(url, output_path)
                    
                    # 检查文件是否下载成功
                    if output_path.exists() and output_path.stat().st_size > 1024*1024:  # 至少1MB
                        print(f"✅ 模型下载成功: {output_path}")
                        file_size = output_path.stat().st_size
                        print(f"📏 文件大小: {file_size / (1024*1024*1024):.2f} GB")
                        download_success = True
                        break
                    else:
                        print(f"❌ 从 {url} 下载失败，尝试下一个URL...")
                        if output_path.exists():
                            output_path.unlink()
                        
                except Exception as e:
                    print(f"❌ 从 {url} 下载失败: {str(e)}")
                    if output_path.exists():
                        output_path.unlink()
                    continue
        
        # 如果直接下载失败，尝试从 Hugging Face Hub 下载并转换
        if not download_success and model_version in model_repo_map:
            try:
                print("📦 尝试从 Hugging Face Hub 下载原始模型...")
                repo_id = model_repo_map[model_version]
                
                print(f"🔄 正在下载 {repo_id}...")
                processor = WhisperProcessor.from_pretrained(repo_id)
                model = WhisperForConditionalGeneration.from_pretrained(repo_id)
                
                # 创建临时目录保存模型
                temp_dir = Path(f"temp_model_{model_version}")
                temp_dir.mkdir(exist_ok=True)
                
                # 保存模型
                model.save_pretrained(temp_dir)
                processor.save_pretrained(temp_dir)
                
                print("⚠️  模型下载完成，但需要手动转换为 GGUF 格式")
                print("ℹ️  注意：这个模型是 PyTorch 格式，需要使用 whisper.cpp 工具转换为 GGUF 格式")
                
                # 清理临时目录
                shutil.rmtree(temp_dir, ignore_errors=True)
                
                # 创建一个占位符文件说明情况
                with open(output_path, 'w') as f:
                    f.write(f"模型 {model_version} 已从 {repo_id} 下载，但需要转换为 GGUF 格式。\n")
                    f.write("请使用 whisper.cpp 工具进行转换。\n")
                
                download_success = True
                
            except Exception as e:
                print(f"❌ 从 Hugging Face Hub 下载失败: {str(e)}")
        
        if not download_success:
            print(f"❌ 无法下载模型 {model_version}，所有下载方式都失败了")
            exit(1)

        # 验证下载的模型文件
        print("🔍 验证下载的模型文件...")
        
        if not output_path.exists():
            print("❌ 错误: 模型文件下载失败")
            exit(1)
            
        file_size = output_path.stat().st_size
        print(f"✅ 模型文件验证成功:")
        print(f"   📁 文件路径: {output_path}")
        print(f"   📏 文件大小: {file_size / (1024*1024):.1f} MB")
        
        # 计算 SHA256 校验和
        sha256_hash = ""
        if file_size > 1048576:  # 大于1MB
            print("🔐 计算 SHA256 校验和...")
            hash_sha256 = hashlib.sha256()
            with open(output_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            sha256_hash = hash_sha256.hexdigest()
            print(f"   🔑 SHA256: {sha256_hash}")
            
            # 保存校验和文件
            sha256_file = output_path.with_suffix('.bin.sha256')
            with open(sha256_file, 'w') as f:
                f.write(f"{sha256_hash}  {model_filename}\n")
        else:
            print(f"⚠️  警告: 文件大小可能不正确 ({file_size} bytes)")

        # 上传模型到 Cloudflare R2
        print("☁️  开始上传模型文件到 Cloudflare R2...")
        
        try:
            # 上传模型文件
            print(f"📤 上传模型文件: {model_filename}")
            with open(output_path, 'rb') as f:
                s3_client.upload_fileobj(f, bucket_name, model_key)
            print("✅ 模型文件上传完成")
            
            # 上传校验和文件
            sha256_file = output_path.with_suffix('.bin.sha256')
            if sha256_file.exists():
                sha256_key = f"models/{model_filename}.sha256"
                print(f"📤 上传校验和文件: {model_filename}.sha256")
                with open(sha256_file, 'rb') as f:
                    s3_client.upload_fileobj(f, bucket_name, sha256_key)
                print("✅ 校验和文件上传完成")
                
        except Exception as e:
            print(f"❌ 上传文件失败: {e}")
            exit(1)

        # 创建模型信息文件
        print("📋 创建模型信息文件...")
        
        # 格式化文件大小
        def format_file_size(size_bytes):
            if size_bytes >= 1024**3:
                return f"{size_bytes / (1024**3):.1f}G"
            elif size_bytes >= 1024**2:
                return f"{size_bytes / (1024**2):.1f}M"
            elif size_bytes >= 1024:
                return f"{size_bytes / 1024:.1f}K"
            else:
                return f"{size_bytes}B"
        
        model_info = {
            "model_name": f"whisper-{model_version}",
            "version": model_version,
            "format": "gguf",
            "file_name": model_filename,
            "uploaded_at": datetime.now(timezone.utc).isoformat(),
            "upload_workflow": workflow_name,
            "run_id": run_id,
            "run_number": run_number,
            "file_size_bytes": file_size,
            "file_size_human": format_file_size(file_size),
            "sha256": sha256_hash if sha256_hash else "unknown",
            "download_url": f"https://velopack.miaostay.com/models/{model_filename}",
            "description": f"Whisper {model_version} GGUF format model for speech recognition",
            "compatible_with": ["whisper.cpp", "llama.cpp", "candle-whisper"],
            "license": "MIT",
            "source": "OpenAI Whisper"
        }
        
        print("📄 模型信息文件内容:")
        print(json.dumps(model_info, indent=2, ensure_ascii=False))
        
        # 上传模型信息文件
        try:
            info_key = f"models/whisper-{model_version}-info.json"
            info_content = json.dumps(model_info, indent=2, ensure_ascii=False)
            s3_client.put_object(
                Bucket=bucket_name,
                Key=info_key,
                Body=info_content.encode('utf-8'),
                ContentType='application/json'
            )
            print("✅ 模型信息文件上传完成")
        except Exception as e:
            print(f"❌ 上传模型信息文件失败: {e}")

        # 列出 R2 中的模型文件
        print("📂 当前 R2 存储桶中的模型文件:")
        try:
            response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix="models/")
            if 'Contents' in response:
                for obj in response['Contents']:
                    size_human = format_file_size(obj['Size'])
                    print(f"   📄 {obj['Key']} ({size_human})")
            else:
                print("   📭 没有找到模型文件")
        except Exception as e:
            print(f"⚠️  列出文件失败: {e}")

        # 生成 GitHub Actions 摘要
        summary_lines = [
            "## 🎉 Whisper 模型上传完成",
            "",
            "### 📋 上传详情",
            f"- **模型版本**: {model_version}",
            f"- **上传时间**: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}",
            f"- **工作流**: {workflow_name}",
            f"- **运行 ID**: {run_id}",
            f"- **运行编号**: {run_number}",
            f"- **操作**: {'强制重新下载并上传' if force_download else '下载并上传新模型'}",
            "",
            "### 📁 文件位置",
            f"- **模型文件**: `models/{model_filename}`",
            f"- **校验和**: `models/{model_filename}.sha256`",
            f"- **信息文件**: `models/whisper-{model_version}-info.json`",
            "",
            "### 📊 文件信息",
            f"- **文件大小**: {format_file_size(file_size)}",
            f"- **SHA256**: `{sha256_hash if sha256_hash else 'unknown'}`",
            "",
            "### 🔗 使用说明",
            "模型文件已上传到 Cloudflare R2，可以在您的应用程序中使用以下 URL 访问:",
            "```",
            f"https://velopack.miaostay.com/models/{model_filename}",
            "```",
            "",
            "### 🛠️ 兼容性",
            "这个模型与以下工具兼容:",
            "- whisper.cpp",
            "- llama.cpp", 
            "- candle-whisper",
            "- 其他支持 GGUF 格式的推理引擎"
        ]
        
        # 写入 GitHub Actions 摘要
        summary_file = os.environ.get('GITHUB_STEP_SUMMARY')
        if summary_file:
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(summary_lines))
            print("📝 GitHub Actions 摘要已生成")
        
        print("🎊 模型处理完成！")
        
        EOF 