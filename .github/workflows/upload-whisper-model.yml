name: ä¸‹è½½å¹¶ä¸Šä¼  Whisper Large V3 Turbo æ¨¡å‹

on:
  workflow_dispatch:
    inputs:
      model_version:
        description: 'æ¨¡å‹ç‰ˆæœ¬ (é»˜è®¤: large-v3-turbo)'
        required: false
        default: 'large-v3-turbo'
        type: string
      force_download:
        description: 'å¼ºåˆ¶é‡æ–°ä¸‹è½½æ¨¡å‹'
        required: false
        default: false
        type: boolean

jobs:
  download-and-upload-model:
    permissions: write-all
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      
    - name: è®¾ç½® Python ç¯å¢ƒ
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: å®‰è£…ä¾èµ–åŒ…
      run: |
        pip install huggingface_hub torch transformers
        
    - name: é…ç½® AWS CLI (ç”¨äº Cloudflare R2)
      run: |
        sudo apt-get update
        sudo apt-get install -y awscli
        
        # é…ç½® AWS CLI è®¿é—® R2
        aws configure set aws_access_key_id ${{ secrets.R2_ACCESS_KEY_ID }}
        aws configure set aws_secret_access_key ${{ secrets.R2_SECRET_ACCESS_KEY }}
        aws configure set region auto
        
    - name: æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
      id: check_model
      env:
        S3_ENDPOINT: ${{ secrets.S3_ENDPOINT }}
        R2_BUCKET: ${{ secrets.R2_BUCKET }}
      run: |
        model_name="ggml-${{ github.event.inputs.model_version }}.bin"
        echo "æ­£åœ¨æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: $model_name"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨äº R2
        if aws s3api head-object --bucket $R2_BUCKET --key "models/$model_name" --endpoint-url $S3_ENDPOINT >/dev/null 2>&1; then
          echo "æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨äº R2"
          echo "exists=true" >> $GITHUB_OUTPUT
          if [ "${{ github.event.inputs.force_download }}" = "true" ]; then
            echo "å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²å¯ç”¨"
            echo "should_download=true" >> $GITHUB_OUTPUT
          else
            echo "è·³è¿‡ä¸‹è½½ï¼Œä½¿ç”¨ç°æœ‰æ–‡ä»¶"
            echo "should_download=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸‹è½½"
          echo "exists=false" >> $GITHUB_OUTPUT
          echo "should_download=true" >> $GITHUB_OUTPUT
        fi
        
    - name: ä¸‹è½½å¹¶è½¬æ¢ Whisper æ¨¡å‹
      if: steps.check_model.outputs.should_download == 'true'
      run: |
        echo "å¼€å§‹ä¸‹è½½ Whisper ${{ github.event.inputs.model_version }} æ¨¡å‹..."
        
        # åˆ›å»ºæ¨¡å‹ä¸‹è½½ç›®å½•
        mkdir -p models
        
        # ä½¿ç”¨ Python è„šæœ¬ä¸‹è½½å’Œè½¬æ¢æ¨¡å‹
        python << 'EOF'
        import os
        import urllib.request
        import urllib.error
        from huggingface_hub import hf_hub_download, login
        import torch
        from transformers import WhisperProcessor, WhisperForConditionalGeneration
        import shutil
        
        model_version = "${{ github.event.inputs.model_version }}"
        print(f"æ­£åœ¨å¤„ç†æ¨¡å‹: {model_version}")
        
        # æ ¹æ®æ¨¡å‹ç‰ˆæœ¬è®¾ç½®ä»“åº“å’Œæ–‡ä»¶å
        model_repo_map = {
            "large-v3-turbo": "openai/whisper-large-v3-turbo",
            "large-v3": "openai/whisper-large-v3",
            "large-v2": "openai/whisper-large-v2",
            "large": "openai/whisper-large",
            "medium": "openai/whisper-medium",
            "small": "openai/whisper-small",
            "base": "openai/whisper-base",
            "tiny": "openai/whisper-tiny"
        }
        
        # GGUF æ ¼å¼æ¨¡å‹çš„ç›´æ¥ä¸‹è½½é“¾æ¥
        gguf_download_urls = {
            "large-v3-turbo": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3-turbo.bin",
                "https://huggingface.co/Demonthos/candle-quantized-whisper-large-v3-turbo/resolve/main/model.gguf"
            ],
            "large-v3": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3.bin"
            ],
            "large-v2": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v2.bin"
            ],
            "large": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large.bin"
            ],
            "medium": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-medium.bin"
            ],
            "small": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-small.bin"
            ],
            "base": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.bin"
            ],
            "tiny": [
                "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.bin"
            ]
        }
        
        output_path = f"./models/ggml-{model_version}.bin"
        download_success = False
        
        # å°è¯•ç›´æ¥ä¸‹è½½ GGUF æ ¼å¼çš„æ¨¡å‹
        if model_version in gguf_download_urls:
            for url in gguf_download_urls[model_version]:
                try:
                    print(f"æ­£åœ¨ä» {url} ä¸‹è½½æ¨¡å‹...")
                    urllib.request.urlretrieve(url, output_path)
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸‹è½½æˆåŠŸ
                    if os.path.exists(output_path) and os.path.getsize(output_path) > 1024*1024:  # è‡³å°‘1MB
                        print(f"æ¨¡å‹ä¸‹è½½æˆåŠŸ: {output_path}")
                        file_size = os.path.getsize(output_path)
                        print(f"æ–‡ä»¶å¤§å°: {file_size / (1024*1024*1024):.2f} GB")
                        download_success = True
                        break
                    else:
                        print(f"ä» {url} ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªURL...")
                        if os.path.exists(output_path):
                            os.remove(output_path)
                        
                except Exception as e:
                    print(f"ä» {url} ä¸‹è½½å¤±è´¥: {str(e)}")
                    if os.path.exists(output_path):
                        os.remove(output_path)
                    continue
        
        # å¦‚æœç›´æ¥ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä» Hugging Face Hub ä¸‹è½½å¹¶è½¬æ¢
        if not download_success and model_version in model_repo_map:
            try:
                print("å°è¯•ä» Hugging Face Hub ä¸‹è½½åŸå§‹æ¨¡å‹...")
                repo_id = model_repo_map[model_version]
                
                # ä¸‹è½½æ¨¡å‹å’Œå¤„ç†å™¨
                print(f"æ­£åœ¨ä¸‹è½½ {repo_id}...")
                processor = WhisperProcessor.from_pretrained(repo_id)
                model = WhisperForConditionalGeneration.from_pretrained(repo_id)
                
                # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜æ¨¡å‹
                temp_dir = f"./temp_model_{model_version}"
                os.makedirs(temp_dir, exist_ok=True)
                
                # ä¿å­˜æ¨¡å‹
                model.save_pretrained(temp_dir)
                processor.save_pretrained(temp_dir)
                
                print("æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œä½†éœ€è¦æ‰‹åŠ¨è½¬æ¢ä¸º GGUF æ ¼å¼")
                print("æ³¨æ„ï¼šè¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch æ ¼å¼ï¼Œéœ€è¦ä½¿ç”¨ whisper.cpp å·¥å…·è½¬æ¢ä¸º GGUF æ ¼å¼")
                
                # æ¸…ç†ä¸´æ—¶ç›®å½•
                shutil.rmtree(temp_dir, ignore_errors=True)
                
                # åˆ›å»ºä¸€ä¸ªå ä½ç¬¦æ–‡ä»¶è¯´æ˜æƒ…å†µ
                with open(output_path, 'w') as f:
                    f.write(f"æ¨¡å‹ {model_version} å·²ä» {repo_id} ä¸‹è½½ï¼Œä½†éœ€è¦è½¬æ¢ä¸º GGUF æ ¼å¼ã€‚\n")
                    f.write("è¯·ä½¿ç”¨ whisper.cpp å·¥å…·è¿›è¡Œè½¬æ¢ã€‚\n")
                
                download_success = True
                
            except Exception as e:
                print(f"ä» Hugging Face Hub ä¸‹è½½å¤±è´¥: {str(e)}")
        
        if not download_success:
            raise Exception(f"æ— æ³•ä¸‹è½½æ¨¡å‹ {model_version}ï¼Œæ‰€æœ‰ä¸‹è½½æ–¹å¼éƒ½å¤±è´¥äº†")
        
        EOF
        
    - name: éªŒè¯ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶
      if: steps.check_model.outputs.should_download == 'true'
      run: |
        model_file="models/ggml-${{ github.event.inputs.model_version }}.bin"
        
        if [ -f "$model_file" ]; then
          echo "æ¨¡å‹æ–‡ä»¶éªŒè¯æˆåŠŸ:"
          ls -lh "$model_file"
          
          # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åˆç†
          file_size=$(stat -c%s "$model_file")
          if [ $file_size -gt 1048576 ]; then  # å¤§äº1MB
            echo "è®¡ç®— SHA256 æ ¡éªŒå’Œ..."
            sha256sum "$model_file" > "$model_file.sha256"
            cat "$model_file.sha256"
          else
            echo "è­¦å‘Š: æ–‡ä»¶å¤§å°å¯èƒ½ä¸æ­£ç¡® ($file_size bytes)"
          fi
        else
          echo "é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸‹è½½å¤±è´¥"
          exit 1
        fi
        
    - name: ä¸Šä¼ æ¨¡å‹åˆ° Cloudflare R2
      if: steps.check_model.outputs.should_download == 'true'
      env:
        S3_ENDPOINT: ${{ secrets.S3_ENDPOINT }}
        R2_BUCKET: ${{ secrets.R2_BUCKET }}
      run: |
        model_file="models/ggml-${{ github.event.inputs.model_version }}.bin"
        sha256_file="$model_file.sha256"
        
        echo "å¼€å§‹ä¸Šä¼ æ¨¡å‹æ–‡ä»¶åˆ° Cloudflare R2..."
        
        # ä¸Šä¼ æ¨¡å‹æ–‡ä»¶
        if [ -f "$model_file" ]; then
          aws s3 cp "$model_file" "s3://$R2_BUCKET/models/" --endpoint-url $S3_ENDPOINT
          echo "æ¨¡å‹æ–‡ä»¶ä¸Šä¼ å®Œæˆ"
        fi
        
        # ä¸Šä¼ æ ¡éªŒå’Œæ–‡ä»¶
        if [ -f "$sha256_file" ]; then
          aws s3 cp "$sha256_file" "s3://$R2_BUCKET/models/" --endpoint-url $S3_ENDPOINT
          echo "æ ¡éªŒå’Œæ–‡ä»¶ä¸Šä¼ å®Œæˆ"
        fi
        
    - name: åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶
      env:
        S3_ENDPOINT: ${{ secrets.S3_ENDPOINT }}
        R2_BUCKET: ${{ secrets.R2_BUCKET }}
      run: |
        model_version="${{ github.event.inputs.model_version }}"
        model_file="models/ggml-$model_version.bin"
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_size=0
        sha256_hash="unknown"
        
        if [ -f "$model_file" ]; then
          file_size=$(stat -c%s "$model_file")
        fi
        
        if [ -f "$model_file.sha256" ]; then
          sha256_hash=$(cat "$model_file.sha256" | cut -d' ' -f1)
        fi
        
        # åˆ›å»ºæ¨¡å‹ä¿¡æ¯ JSON æ–‡ä»¶
        cat > model_info.json << EOF
        {
          "model_name": "whisper-$model_version",
          "version": "$model_version",
          "format": "gguf",
          "file_name": "ggml-$model_version.bin",
          "uploaded_at": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
          "upload_workflow": "${{ github.workflow }}",
          "run_id": "${{ github.run_id }}",
          "run_number": "${{ github.run_number }}",
          "file_size_bytes": $file_size,
          "file_size_human": "$(numfmt --to=iec $file_size 2>/dev/null || echo "unknown")",
          "sha256": "$sha256_hash",
          "download_url": "https://r2.your-domain.com/models/ggml-$model_version.bin",
          "description": "Whisper $model_version GGUF format model for speech recognition",
          "compatible_with": ["whisper.cpp", "llama.cpp", "candle-whisper"],
          "license": "MIT",
          "source": "OpenAI Whisper"
        }
        EOF
        
        echo "æ¨¡å‹ä¿¡æ¯æ–‡ä»¶å†…å®¹:"
        cat model_info.json | python -m json.tool 2>/dev/null || cat model_info.json
        
        # ä¸Šä¼ æ¨¡å‹ä¿¡æ¯æ–‡ä»¶
        aws s3 cp model_info.json "s3://$R2_BUCKET/models/whisper-$model_version-info.json" --endpoint-url $S3_ENDPOINT
        
        echo "æ¨¡å‹ä¿¡æ¯æ–‡ä»¶ä¸Šä¼ å®Œæˆ"
        
    - name: åˆ—å‡º R2 ä¸­çš„æ¨¡å‹æ–‡ä»¶
      env:
        S3_ENDPOINT: ${{ secrets.S3_ENDPOINT }}
        R2_BUCKET: ${{ secrets.R2_BUCKET }}
      run: |
        echo "å½“å‰ R2 å­˜å‚¨æ¡¶ä¸­çš„æ¨¡å‹æ–‡ä»¶:"
        aws s3 ls "s3://$R2_BUCKET/models/" --endpoint-url $S3_ENDPOINT --recursive --human-readable
        
    - name: ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
      run: |
        echo "## ğŸ‰ Whisper æ¨¡å‹ä¸Šä¼ å®Œæˆ" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“‹ ä¸Šä¼ è¯¦æƒ…" >> $GITHUB_STEP_SUMMARY
        echo "- **æ¨¡å‹ç‰ˆæœ¬**: ${{ github.event.inputs.model_version }}" >> $GITHUB_STEP_SUMMARY
        echo "- **ä¸Šä¼ æ—¶é—´**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
        echo "- **å·¥ä½œæµ**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
        echo "- **è¿è¡Œ ID**: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
        echo "- **è¿è¡Œç¼–å·**: ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.check_model.outputs.should_download }}" = "true" ]; then
          echo "- **æ“ä½œ**: ä¸‹è½½å¹¶ä¸Šä¼ æ–°æ¨¡å‹" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **æ“ä½œ**: æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“ æ–‡ä»¶ä½ç½®" >> $GITHUB_STEP_SUMMARY
        echo "- **æ¨¡å‹æ–‡ä»¶**: \`models/ggml-${{ github.event.inputs.model_version }}.bin\`" >> $GITHUB_STEP_SUMMARY
        echo "- **æ ¡éªŒå’Œ**: \`models/ggml-${{ github.event.inputs.model_version }}.bin.sha256\`" >> $GITHUB_STEP_SUMMARY
        echo "- **ä¿¡æ¯æ–‡ä»¶**: \`models/whisper-${{ github.event.inputs.model_version }}-info.json\`" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ”— ä½¿ç”¨è¯´æ˜" >> $GITHUB_STEP_SUMMARY
        echo "æ¨¡å‹æ–‡ä»¶å·²ä¸Šä¼ åˆ° Cloudflare R2ï¼Œå¯ä»¥åœ¨æ‚¨çš„åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ä»¥ä¸‹ URL è®¿é—®:" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        echo "https://your-r2-domain.com/models/ggml-${{ github.event.inputs.model_version }}.bin" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ› ï¸ å…¼å®¹æ€§" >> $GITHUB_STEP_SUMMARY
        echo "è¿™ä¸ªæ¨¡å‹ä¸ä»¥ä¸‹å·¥å…·å…¼å®¹:" >> $GITHUB_STEP_SUMMARY
        echo "- whisper.cpp" >> $GITHUB_STEP_SUMMARY
        echo "- llama.cpp" >> $GITHUB_STEP_SUMMARY
        echo "- candle-whisper" >> $GITHUB_STEP_SUMMARY
        echo "- å…¶ä»–æ”¯æŒ GGUF æ ¼å¼çš„æ¨ç†å¼•æ“" >> $GITHUB_STEP_SUMMARY 